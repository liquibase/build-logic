name: Generate and upload Fossa Report to s3

on:
  workflow_call:
    inputs:
      version_number_for_report_generation:
        type: string
        required: true

jobs:
  fossa-scan:
    runs-on: ubuntu-latest
    env:
      FOSSA_API_KEY: ${{ secrets.FOSSA_API_KEY }}
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}

      - name: Set up JDK for Datical
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'
          cache: 'maven'
          server-id: datical
          server-username: REPO_MAVEN_USER
          server-password: REPO_MAVEN_PASSWORD

      - name: Set up JDK for Liquibase
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '17'

        #look for dependencies in maven
      - name: maven-settings-xml-action for Liquibase
        uses: whelk-io/maven-settings-xml-action@v22
        with:
          repositories: |
            [
              {
                "id": "liquibase",
                "url": "https://maven.pkg.github.com/liquibase/liquibase",
                "releases": {
                  "enabled": "false"
                },
                "snapshots": {
                  "enabled": "true",
                  "updatePolicy": "always"
                }
              },
              {
                "id": "liquibase-pro",
                "url": "https://maven.pkg.github.com/liquibase/liquibase-pro",
                "releases": {
                  "enabled": "false"
                },
                "snapshots": {
                  "enabled": "true",
                  "updatePolicy": "always"
                }
              }
            ]
          servers: |
            [
              {
                "id": "liquibase-pro",
                "username": "liquibot",
                "password": "${{ secrets.LIQUIBOT_PAT }}"
              },
              {
                "id": "liquibase",
                "username": "liquibot",
                "password": "${{ secrets.LIQUIBOT_PAT }}"
              }
            ]

      - name: Get the commit sha
        id: get_commit_sha
        run: |
          commit_sha=`(git rev-parse HEAD)`
          echo "commit_sha=${commit_sha}" >> $GITHUB_OUTPUT

      - name: Get repository name
        id: get_repo_name
        run: echo "repo_name=${{ github.event.repository.name }}" >> $GITHUB_OUTPUT

      - name: Setup Fossa CLI
        run: |
          curl -H 'Cache-Control: no-cache' https://raw.githubusercontent.com/fossas/fossa-cli/master/install-latest.sh | bash
          export FOSSA_API_KEY="${{ secrets.FOSSA_API_KEY }}"

      - name: Analyze project
        run: fossa analyze

      - name: Generate report
        run: |
          # Run JSON report
          fossa report attribution --format json > fossa.json
          
          csv_filename="${{ steps.get_repo_name.outputs.repo_name }}.csv"
          
          # Extract fields from the JSON and create a CSV report.
          echo "Title,Version,Declared License,Package Homepage" > $csv_filename
          
          jq -r '
          (.directDependencies + .deepDependencies)[] |
          [
            .title,
            .version,
            (.licenses | map(.name) | join(";")),
            .projectUrl
          ] |
          @csv
          ' fossa.json >> $csv_filename

        # Upload the report to S3 with conditional paths:
        # - If the repository is 'datical-service', upload to a version-specific folder
        # - Otherwise, upload to the generic 'raw_reports' folder
      - name: Upload report to S3
        if: always()
        run: |
          csv_filename="${{ steps.get_repo_name.outputs.repo_name }}.csv"
          if [ "${{ steps.get_repo_name.outputs.repo_name }}" == "datical-service" ]; then
            aws s3 cp $csv_filename s3://liquibaseorg-origin/enterprise_fossa_report/${{ inputs.version_number_for_report_generation }}/
          else
          aws s3 cp $csv_filename s3://liquibaseorg-origin/enterprise_fossa_report/raw_reports/
          fi
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.LIQUIBASEORIGIN_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.LIQUIBASEORIGIN_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1


      - name: Upload to build page
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: fossa-reports
          path: |
            /home/runner/work/${{ steps.set_csv_filename.outputs.csv_filename }}

  combine-fossa-reports:
    runs-on: ubuntu-latest
    if: ${{ steps.get_repo_name.outputs.repo_name }}" == 'datical-service'
    needs: fossa-scan
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          repository: liquibase/build-logic
          ref: DAT-18919
          path: build-logic

      - name: Set up AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.LIQUIBASEORIGIN_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.LIQUIBASEORIGIN_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Download reports from S3 and Rearrange CSV files
        run: |
          # Create a directory to store downloaded reports from S3
          mkdir -p /home/runner/work/enterprise/fossa_reports_s3

          # Download all files from the specified S3 bucket to the created directory
          aws s3 cp --recursive s3://liquibaseorg-origin/enterprise_fossa_report/raw_reports /home/runner/work/enterprise/fossa_reports_s3/

          # List the contents of the directory to confirm successful download
          ls -l /home/runner/work/enterprise/fossa_reports_s3

          # Define an array of CSV file names
          csv_files=("DaticalDB-installer" "drivers" "protoclub" "datical-sqlparser" "storedlogic" "AppDBA" "liquibase-bundle" "liquibase")

          # Loop through each CSV file and remove headers again for combine report generation
          for file in "${csv_files[@]}"; do
            tail -n +1 /home/runner/work/enterprise/fossa_reports_s3/${file}.csv >> /home/runner/work/enterprise/fossa_reports_s3/${file}_no_header.csv
          done

          # Concatenate all CSV files without headers, sort, and remove duplicates
          cat /home/runner/work/enterprise/fossa_reports_s3/*_no_header.csv | sort | uniq > /home/runner/work/enterprise/fossa_reports_s3/enterprise_unique.csv

          # Add a header to the final CSV file, placing it above the sorted and unique data
          echo 'Title,Version,Declared License,Package Homepage' | cat - /home/runner/work/enterprise/fossa_reports_s3/enterprise_unique.csv > temp && mv temp /home/runner/work/enterprise/fossa_reports_s3/enterprise_unique.csv

          ls -l $GITHUB_WORKSPACE

          # Read ignored dependencies from a file
          ignoredLibsFile=$(cat $GITHUB_WORKSPACE/build-logic/.github/workflows/ignore_dependencies_fossa.txt)

          # Split the ignored dependencies into an array
          IFS=',' read -r -a ignoredLibs <<< "$ignoredLibsFile"

          # Create a temporary file
          tempfile=$(mktemp)

          # Build the grep command to filter out ignored dependencies
          grepCmd="grep -iv"
          for lib in "${ignoredLibs[@]}"; do
            grepCmd="$grepCmd -e \"$lib\""
          done

          # Process the FOSSA report to remove ignored dependencies
          cat /home/runner/work/enterprise/fossa_reports_s3/enterprise_unique.csv | eval $grepCmd > enterprise_report.csv


      - name: Upload CSV to Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: enterprise_report
          path: ${{ inputs.version_number_for_report_generation }}

      - name: Upload merged CSV to S3
        if: always()
        run: aws s3 cp enterprise_report.csv s3://liquibaseorg-origin/enterprise_fossa_report/${{ inputs.version_number_for_report_generation }}/enterprise_report_${{ inputs.version_number_for_report_generation }}.csv